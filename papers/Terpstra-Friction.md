# Think before you click: how reflective patterns contribute to privacy

## My Background

I work as a product manager for the Dutch National Research & Education Network, SURF [^https://www.surf.nl/]. Also, I'm an external PhD student at the Tilburg Institute for Law, Technology & Society. My research brings together different fields related to online privacy, such as law and Human Computer Interaction. Specifically, I argue devices and interfaces should be designed with more friction in them, to allow users more opportunity for reflection about (the consequences of) privacy choices [1].

## Topic

Currently, privacy and data protection regulations aim to strengthen individual's rights by forcing organisations who process personal data to be transparant about what data they process and for which purposes, and giving users more control over their data. However, privacy policies do not accomplish their goal of effectively informing users, nor does take-it-or-leave-it consent on the one hand, or using nudges and/or dark patterns to make users click something unconsciously on the other hand give users more meaningful control [2].

In order to learn and reason about how using certain digital products might affect one's privacy (i.e., to make 'good' privacy decisions) individuals need to use reflective thinking. Reflective thinking, or (critical) reflection, is the examination of previous experiences and assumptions as input for future actions, assumptions and decisions. It starts with an awareness that existing assumptions may need to be (re-)examined, initiated by a disorienting dilemma.

Individuals should thus be encouraged – through the design of the products and services they use – to make individual choices and maintain a moral position by using their reflective capabilities to reflect on how they think and feel about privacy before, during, and after interacting with digital technology. This requires at least three components: (1) a way to challenge one’s habitual behaviour and thoughts, e.g., through friction; (2) a phase of reflective thinking about one’s own privacy behaviour and its consequences, supported by the digital product, e.g., by giving the user relevant information on privacy issues; (3) meaningful controls: the possibility to put newly learned thoughts and ideas into action.

Although there are some definitions of the concept of 'friction', it is as of yet unclear what kind of friction could lead to reflection. Recently, I finished a study into Online Proctoring (monitoring students during an online exam to prevent and detect cheating and fraud) [3], which might provide some interesting insights. In this study, I used Helen Nissenbaum's definition of privacy as Contextual Integrity [4] to make information flows very explicit.

Contextual Integrity states that what is considered appropriate information flowing from one actor to another is determined by context-specific norms, which originate from the social, cultural, and legal domain. Such norms can be explicit, for instance codified in laws and regulations, as well as implicit, such as social norms prescribing acceptable behaviour in certain social settings. Information flows consist of five parameters: (1) the sender; the actor responsible for sending out the information, (2) the recipient; the actor receiving the information, (3) the type of information; the actual information being transmitted, (4) the subject; the actor to whom the information relates and (5) the transmission principle; under which conditions or constraints the information is allowed to flow. For example, an (acceptable) information flow might be a student (sender) sharing with their study advisor (recipient) a particular dislike about a certain teacher (information type, subject), if the study advisor guarantees to keep this information between them (transmission principle). Whenever one (or more) of these five parameters are not in line with a person’s norm, a privacy violation occurs.

During the workshop, I propose to use above mentioned theories and lead the discussion on devising a more detailed and practical realisation of the concept of 'friction'. It would even be better if additionally, a set of design principles would emerge. Both could be of high value to the W3C working group working on a proposal to improve user control over their personal data.

# References
1. See https://firstmonday.org/ojs/index.php/fm/article/view/9358
2. See https://repository.ubn.ru.nl/handle/2066/246490
3. Manuscript is currently in review.
4. See https://www.sup.org/books/title/?id=8862
